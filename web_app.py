#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ¯”èµ›ä¿¡æ¯çˆ¬è™«Webç®¡ç†ç•Œé¢
æä¾›çˆ¬è™«ä»»åŠ¡ç®¡ç†ã€å®šæ—¶è°ƒåº¦ã€çŠ¶æ€ç›‘æ§ç­‰åŠŸèƒ½
"""

import os
import json
import datetime
import threading
import subprocess
from pathlib import Path
from flask import Flask, render_template, request, jsonify, send_from_directory
from flask_cors import CORS
from flask_socketio import SocketIO, emit
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.triggers.cron import CronTrigger
import psutil

# å¯¼å…¥çˆ¬è™«æ¨¡å—
try:
    from crawlers.baidu_crawler import crawl_baidu
    from crawlers.aliyun_crawler import crawl_aliyun
    from crawlers.tencent_crawler import crawl_tencent
    from crawlers.wechat_crawler import crawl_wechat
    from feishu_api import update_all_competition_status, get_all_records
except ImportError as e:
    print(f"å¯¼å…¥çˆ¬è™«æ¨¡å—å¤±è´¥: {e}")

# åˆå§‹åŒ–Flaskåº”ç”¨
app = Flask(__name__)
app.config['SECRET_KEY'] = 'crawler_web_ui_secret_key'
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

# åˆå§‹åŒ–è°ƒåº¦å™¨
scheduler = BackgroundScheduler()
scheduler.start()

# å…¨å±€å˜é‡
task_status = {
    'baidu': {'status': 'idle', 'last_run': None, 'message': ''},
    'aliyun': {'status': 'idle', 'last_run': None, 'message': ''},
    'tencent': {'status': 'idle', 'last_run': None, 'message': ''},
    'wechat': {'status': 'idle', 'last_run': None, 'message': ''},
    'update_status': {'status': 'idle', 'last_run': None, 'message': ''}
}

running_tasks = {}
scheduled_jobs = {}

# çˆ¬è™«å‡½æ•°æ˜ å°„
CRAWLER_FUNCTIONS = {
    'baidu': crawl_baidu,
    'aliyun': crawl_aliyun,
    'tencent': crawl_tencent,
    'wechat': crawl_wechat,
    'update_status': update_all_competition_status
}

PLATFORM_NAMES = {
    'baidu': 'ç™¾åº¦AI Studio',
    'aliyun': 'é˜¿é‡Œå¤©æ± ',
    'tencent': 'è…¾è®¯CSDN',
    'wechat': 'å¾®ä¿¡å…¬ä¼—å·',
    'update_status': 'çŠ¶æ€æ›´æ–°'
}

def get_all_competitions():
    """è·å–æ‰€æœ‰æ¯”èµ›æ•°æ®"""
    try:
        records = get_all_records()
        competitions = []
        
        for record in records:
            fields = record.get('fields', {})
            competition = {
                'id': record.get('record_id', ''),
                'name': fields.get('æ¯”èµ›åç§°', ''),
                'link': fields.get('æ¯”èµ›é“¾æ¥', {}).get('link', '') if isinstance(fields.get('æ¯”èµ›é“¾æ¥'), dict) else str(fields.get('æ¯”èµ›é“¾æ¥', '')),
                'status': fields.get('æ¯”èµ›çŠ¶æ€', ''),
                'start_date': fields.get('æ¯”èµ›å¼€å§‹æ—¶é—´', ''),
                'end_date': fields.get('æ¯”èµ›ç»“æŸæ—¶é—´', ''),
                'platform': fields.get('å¹³å°', ''),
                'difficulty': fields.get('éš¾åº¦ç­‰çº§', []),
                'type': fields.get('æ¯”èµ›ç±»å‹', []),
                'prize': fields.get('å¥–é‡‘', ''),
                'participants': fields.get('å‚ä¸äººæ•°', '')
            }
            competitions.append(competition)
        
        return competitions
    except Exception as e:
        print(f"è·å–æ¯”èµ›æ•°æ®å¤±è´¥: {e}")
        return []

def filter_ongoing_competitions(competitions, current_date):
    """ç­›é€‰æœªç»“æŸçš„æ¯”èµ›"""
    try:
        current_dt = datetime.datetime.strptime(current_date, '%Y-%m-%d')
        ongoing = []
        
        for comp in competitions:
            end_date_str = comp.get('end_date', '')
            if end_date_str:
                try:
                    # å°è¯•è§£æç»“æŸæ—¶é—´
                    if isinstance(end_date_str, str) and end_date_str.strip():
                        # å¤„ç†ä¸åŒçš„æ—¥æœŸæ ¼å¼
                        for fmt in ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d', '%Y/%m/%d', '%m/%d/%Y']:
                            try:
                                end_dt = datetime.datetime.strptime(end_date_str.strip(), fmt)
                                if end_dt.date() >= current_dt.date():
                                    ongoing.append(comp)
                                break
                            except ValueError:
                                continue
                except:
                    # å¦‚æœæ— æ³•è§£ææ—¥æœŸï¼Œä¿ç•™æ¯”èµ›
                    ongoing.append(comp)
            else:
                # å¦‚æœæ²¡æœ‰ç»“æŸæ—¥æœŸï¼Œä¿ç•™æ¯”èµ›
                ongoing.append(comp)
        
        return ongoing
    except Exception as e:
        print(f"ç­›é€‰æ¯”èµ›å¤±è´¥: {e}")
        return competitions

def run_crawler_task(platform, filter_date=None):
    """è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    try:
        task_status[platform]['status'] = 'running'
        task_status[platform]['message'] = f'æ­£åœ¨çˆ¬å–{PLATFORM_NAMES[platform]}...'
        socketio.emit('task_status_update', task_status)
        
        # æ‰§è¡Œçˆ¬è™«å‡½æ•°
        if filter_date and platform != 'update_status':
            # å¦‚æœæœ‰ç­›é€‰æ—¥æœŸï¼Œéœ€è¦ä¿®æ”¹çˆ¬è™«å‡½æ•°æ”¯æŒæ—¥æœŸç­›é€‰
            # è¿™é‡Œæš‚æ—¶è¿˜æ˜¯è°ƒç”¨åŸå‡½æ•°ï¼Œåç»­ä¼šä¿®æ”¹çˆ¬è™«é€»è¾‘
            CRAWLER_FUNCTIONS[platform]()
        else:
            CRAWLER_FUNCTIONS[platform]()
        
        task_status[platform]['status'] = 'completed'
        task_status[platform]['last_run'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        task_status[platform]['message'] = f'{PLATFORM_NAMES[platform]}çˆ¬å–å®Œæˆ'
        
        # çˆ¬å–å®Œæˆåå‘é€æ‰€æœ‰æ¯”èµ›æ•°æ®
        competitions = get_all_competitions()
        socketio.emit('competitions_updated', competitions)
        
    except Exception as e:
        task_status[platform]['status'] = 'error'
        task_status[platform]['message'] = f'{PLATFORM_NAMES[platform]}çˆ¬å–å¤±è´¥: {str(e)}'
    
    finally:
        if platform in running_tasks:
            del running_tasks[platform]
        socketio.emit('task_status_update', task_status)

@app.route('/')
def index():
    """ä¸»é¡µé¢"""
    return render_template('index.html')

@app.route('/api/status')
def get_status():
    """è·å–æ‰€æœ‰ä»»åŠ¡çŠ¶æ€"""
    return jsonify({
        'task_status': task_status,
        'scheduled_jobs': list(scheduled_jobs.keys()),
        'system_info': {
            'cpu_percent': psutil.cpu_percent(),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent if os.name != 'nt' else psutil.disk_usage('C:').percent
        }
    })

@app.route('/api/run/<platform>', methods=['POST'])
def run_task(platform):
    """æ‰‹åŠ¨è¿è¡Œçˆ¬è™«ä»»åŠ¡"""
    if platform not in CRAWLER_FUNCTIONS:
        return jsonify({'success': False, 'message': 'ä¸æ”¯æŒçš„å¹³å°'}), 400
    
    if platform in running_tasks:
        return jsonify({'success': False, 'message': 'ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­'}), 400
    
    # åœ¨åå°çº¿ç¨‹ä¸­è¿è¡Œä»»åŠ¡
    thread = threading.Thread(target=run_crawler_task, args=(platform,))
    thread.daemon = True
    thread.start()
    
    running_tasks[platform] = thread
    
    return jsonify({'success': True, 'message': f'å¼€å§‹æ‰§è¡Œ{PLATFORM_NAMES[platform]}çˆ¬å–ä»»åŠ¡'})

@app.route('/api/run/all', methods=['POST'])
def run_all_tasks():
    """è¿è¡Œæ‰€æœ‰çˆ¬è™«ä»»åŠ¡"""
    if any(platform in running_tasks for platform in CRAWLER_FUNCTIONS.keys()):
        return jsonify({'success': False, 'message': 'æœ‰ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­'}), 400
    
    def run_all():
        platforms = ['baidu', 'aliyun', 'tencent', 'wechat', 'update_status']
        for platform in platforms:
            if platform not in running_tasks:
                run_crawler_task(platform)
                # ç­‰å¾…é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                if platform != 'update_status':
                    import time
                    time.sleep(5)
    
    thread = threading.Thread(target=run_all)
    thread.daemon = True
    thread.start()
    
    return jsonify({'success': True, 'message': 'å¼€å§‹æ‰§è¡Œå…¨å¹³å°çˆ¬å–ä»»åŠ¡'})

def run_all_crawlers():
    """ä¾›å®šæ—¶ä»»åŠ¡è°ƒç”¨çš„å…¨å¹³å°çˆ¬å–å‡½æ•°"""
    platforms = ['baidu', 'aliyun', 'tencent', 'wechat', 'update_status']
    for platform in platforms:
        try:
            run_crawler_task(platform)
            # ç­‰å¾…é—´éš”ï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if platform != 'update_status':
                import time
                time.sleep(5)
        except Exception as e:
            print(f"å®šæ—¶ä»»åŠ¡æ‰§è¡Œ{platform}å¤±è´¥: {e}")

@app.route('/api/schedule', methods=['POST'])
def add_schedule():
    """æ·»åŠ å®šæ—¶ä»»åŠ¡"""
    data = request.get_json()
    
    task_name = data.get('taskName')
    frequency = data.get('frequency')
    execute_time = data.get('executeTime')
    weekday = data.get('weekday')
    monthday = data.get('monthday')
    
    if not all([task_name, frequency, execute_time]):
        return jsonify({'success': False, 'message': 'å‚æ•°ä¸å®Œæ•´'}), 400
    
    if frequency not in ['daily', 'weekly', 'monthly']:
        return jsonify({'success': False, 'message': 'ä¸æ”¯æŒçš„é¢‘ç‡ç±»å‹'}), 400
    
    try:
        # è§£ææ‰§è¡Œæ—¶é—´
        time_parts = execute_time.split(':')
        if len(time_parts) != 2:
            return jsonify({'success': False, 'message': 'æ—¶é—´æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºHH:MM'}), 400
        
        hour, minute = time_parts
        hour = int(hour)
        minute = int(minute)
        
        if not (0 <= hour <= 23 and 0 <= minute <= 59):
            return jsonify({'success': False, 'message': 'æ—¶é—´èŒƒå›´é”™è¯¯'}), 400
        
        # ç”Ÿæˆå”¯ä¸€çš„job_id
        job_id = f"{task_name}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ ¹æ®é¢‘ç‡è®¾ç½®è§¦å‘å™¨å‚æ•°
        trigger_kwargs = {
            'hour': hour,
            'minute': minute
        }
        
        if frequency == 'weekly':
            if not weekday:
                return jsonify({'success': False, 'message': 'å‘¨ä»»åŠ¡éœ€è¦æŒ‡å®šæ˜ŸæœŸå‡ '}), 400
            trigger_kwargs['day_of_week'] = int(weekday)
        elif frequency == 'monthly':
            if not monthday:
                return jsonify({'success': False, 'message': 'æœˆä»»åŠ¡éœ€è¦æŒ‡å®šæ—¥æœŸ'}), 400
            trigger_kwargs['day'] = int(monthday)
        
        # æ·»åŠ å®šæ—¶ä»»åŠ¡ - é»˜è®¤è¿è¡Œæ‰€æœ‰çˆ¬è™«
        scheduler.add_job(
            func=run_all_crawlers,
            trigger=CronTrigger(**trigger_kwargs),
            id=job_id,
            replace_existing=True
        )
        
        scheduled_jobs[job_id] = {
            'task_name': task_name,
            'frequency': frequency,
            'execute_time': execute_time,
            'weekday': weekday if frequency == 'weekly' else None,
            'monthday': monthday if frequency == 'monthly' else None,
            'created_at': datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        return jsonify({'success': True, 'message': f'å®šæ—¶ä»»åŠ¡ {task_name} æ·»åŠ æˆåŠŸ'})
        
    except Exception as e:
        return jsonify({'success': False, 'message': f'æ·»åŠ å®šæ—¶ä»»åŠ¡å¤±è´¥: {str(e)}'}), 500

@app.route('/api/schedule/<job_id>', methods=['DELETE'])
def remove_schedule(job_id):
    """åˆ é™¤å®šæ—¶ä»»åŠ¡"""
    try:
        scheduler.remove_job(job_id)
        if job_id in scheduled_jobs:
            del scheduled_jobs[job_id]
        return jsonify({'success': True, 'message': f'å®šæ—¶ä»»åŠ¡ {job_id} åˆ é™¤æˆåŠŸ'})
    except Exception as e:
        return jsonify({'success': False, 'message': f'åˆ é™¤å®šæ—¶ä»»åŠ¡å¤±è´¥: {str(e)}'}), 500

@app.route('/api/logs')
def get_logs():
    """è·å–æ—¥å¿—æ–‡ä»¶åˆ—è¡¨"""
    logs_dir = Path('logs')
    if not logs_dir.exists():
        return jsonify({'logs': []})
    
    log_files = []
    for log_file in logs_dir.glob('*.log'):
        stat = log_file.stat()
        log_files.append({
            'name': log_file.name,
            'size': stat.st_size,
            'modified': datetime.datetime.fromtimestamp(stat.st_mtime).strftime('%Y-%m-%d %H:%M:%S')
        })
    
    return jsonify({'logs': sorted(log_files, key=lambda x: x['modified'], reverse=True)})

@app.route('/api/logs/<filename>')
def get_log_content(filename):
    """è·å–æ—¥å¿—æ–‡ä»¶å†…å®¹"""
    logs_dir = Path('logs')
    log_file = logs_dir / filename
    
    if not log_file.exists():
        return jsonify({'success': False, 'message': 'æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨'}), 404
    
    try:
        with open(log_file, 'r', encoding='utf-8') as f:
            lines = f.readlines()
            # åªè¿”å›æœ€å1000è¡Œ
            content = ''.join(lines[-1000:])
        
        return jsonify({'success': True, 'content': content})
    except Exception as e:
        return jsonify({'success': False, 'message': f'è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {str(e)}'}), 500

@app.route('/api/competitions')
def get_competitions():
    """è·å–æ‰€æœ‰æ¯”èµ›æ•°æ®"""
    try:
        competitions = get_all_competitions()
        return jsonify({
            'success': True,
            'data': competitions,
            'total': len(competitions)
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–æ¯”èµ›æ•°æ®å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/competitions/ongoing', methods=['POST'])
def get_ongoing_competitions():
    """è·å–æœªç»“æŸçš„æ¯”èµ›æ•°æ®"""
    try:
        data = request.get_json()
        current_date = data.get('date', datetime.datetime.now().strftime('%Y-%m-%d'))
        
        all_competitions = get_all_competitions()
        ongoing_competitions = filter_ongoing_competitions(all_competitions, current_date)
        
        return jsonify({
            'success': True,
            'data': ongoing_competitions,
            'total': len(ongoing_competitions),
            'filter_date': current_date
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'è·å–æœªç»“æŸæ¯”èµ›æ•°æ®å¤±è´¥: {str(e)}'
        }), 500

@app.route('/api/run/filtered', methods=['POST'])
def run_filtered_crawl():
    """è¿è¡Œç­›é€‰çˆ¬å–ä»»åŠ¡"""
    try:
        data = request.get_json()
        platforms = data.get('platforms', [])
        filter_date = data.get('date')
        
        if not platforms:
            return jsonify({
                'success': False,
                'message': 'è¯·é€‰æ‹©è¦çˆ¬å–çš„å¹³å°'
            }), 400
        
        if not filter_date:
            return jsonify({
                'success': False,
                'message': 'è¯·æä¾›ç­›é€‰æ—¥æœŸ'
            }), 400
        
        # å¯åŠ¨çˆ¬å–ä»»åŠ¡
        for platform in platforms:
            if platform in CRAWLER_FUNCTIONS and platform not in running_tasks:
                running_tasks[platform] = True
                thread = threading.Thread(target=run_crawler_task, args=(platform, filter_date))
                thread.daemon = True
                thread.start()
        
        return jsonify({
            'success': True,
            'message': f'å·²å¯åŠ¨ {len(platforms)} ä¸ªå¹³å°çš„ç­›é€‰çˆ¬å–ä»»åŠ¡',
            'platforms': platforms,
            'filter_date': filter_date
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'message': f'å¯åŠ¨ç­›é€‰çˆ¬å–å¤±è´¥: {str(e)}'
        }), 500

@socketio.on('connect')
def handle_connect():
    """WebSocketè¿æ¥å¤„ç†"""
    emit('task_status_update', task_status)

if __name__ == '__main__':
    # åˆ›å»ºtemplatesç›®å½•
    templates_dir = Path('templates')
    templates_dir.mkdir(exist_ok=True)
    
    # åˆ›å»ºstaticç›®å½•
    static_dir = Path('static')
    static_dir.mkdir(exist_ok=True)
    
    print("ğŸš€ çˆ¬è™«Webç®¡ç†ç•Œé¢å¯åŠ¨")
    print("ğŸ“± è®¿é—®åœ°å€: http://localhost:5000")
    
    socketio.run(app, host='0.0.0.0', port=5000, debug=True)