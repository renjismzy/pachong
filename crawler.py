import requests
import json
from bs4 import BeautifulSoup
import argparse
import re
import time
import sys
import os
import datetime
from dotenv import load_dotenv
load_dotenv()
app_id = os.getenv('FEISHU_APP_ID')
app_secret = os.getenv('FEISHU_APP_SECRET')
deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')
app_token = 'XgmKbtiiUa8mLhsHDsQcNQKBnVg'
table_id = 'tblgA8hErJNkgZGm'
def update_all_competition_status():
    """æ›´æ–°æ‰€æœ‰æ¯”èµ›çš„çŠ¶æ€ï¼Œå°†å·²ç»“æŸçš„æ¯”èµ›æ ‡è®°ä¸ºå·²ç»“æŸ
    
    Returns:
        tuple: (æ›´æ–°æˆåŠŸæ•°é‡, æ›´æ–°å¤±è´¥æ•°é‡)
    """
    print("ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰æ¯”èµ›çŠ¶æ€...")
    
    # è·å–æ‰€æœ‰è®°å½•
    all_records = get_all_records()
    if not all_records:
        print("âŒ æ— æ³•è·å–è¡¨æ ¼è®°å½•")
        return 0, 0
    
    success_count = 0
    failed_count = 0
    
    for record in all_records:
        try:
            record_id = record.get('record_id')
            fields = record.get('fields', {})
            
            # è·å–æ¯”èµ›åç§°ç”¨äºæ—¥å¿—
            name = fields.get('æ¯”èµ›åç§°', 'Unknown')
            
            # è·å–å½“å‰çŠ¶æ€
            current_status = fields.get('æ¯”èµ›çŠ¶æ€', 'è¿›è¡Œä¸­')
            
            # å¦‚æœå·²ç»æ˜¯å·²ç»“æŸçŠ¶æ€ï¼Œè·³è¿‡
            if current_status == 'å·²ç»“æŸ':
                continue
            
            # è¿™é‡Œéœ€è¦ä»æ¯”èµ›é“¾æ¥è·å–è¯¦ç»†ä¿¡æ¯æ¥åˆ¤æ–­æ˜¯å¦å·²ç»“æŸ
            # ç”±äºæˆ‘ä»¬ç§»é™¤äº†æ¯”èµ›æ—¶é—´å­—æ®µï¼Œéœ€è¦é‡æ–°è·å–æ¯”èµ›è¯¦æƒ…
            link_field = fields.get('æ¯”èµ›é“¾æ¥', {})
            if isinstance(link_field, dict):
                link = link_field.get('link', '')
            else:
                link = str(link_field) if link_field else ''
            
            if link:
                # è·å–æ¯”èµ›è¯¦æƒ…
                participants, prize, start_date, end_date = get_details(link)
                
                # åˆ¤æ–­æ¯”èµ›çŠ¶æ€
                new_status = determine_competition_status(start_date, end_date)
                
                # å¦‚æœçŠ¶æ€éœ€è¦æ›´æ–°ä¸ºå·²ç»“æŸ
                if new_status == 'å·²ç»“æŸ' and current_status != 'å·²ç»“æŸ':
                    if update_record_status(record_id, 'å·²ç»“æŸ'):
                        print(f"âœ… å·²æ›´æ–°æ¯”èµ›çŠ¶æ€: {name} -> å·²ç»“æŸ")
                        success_count += 1
                    else:
                        print(f"âŒ æ›´æ–°å¤±è´¥: {name}")
                        failed_count += 1
                    
                    # æ·»åŠ å»¶è¿Ÿé¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(0.5)
            
        except Exception as e:
            print(f"âš ï¸ å¤„ç†è®°å½•æ—¶å‡ºé”™: {e}")
            failed_count += 1
    
    print(f"ğŸ“Š çŠ¶æ€æ›´æ–°å®Œæˆ: æˆåŠŸ {success_count} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")
    return success_count, failed_count

def analyze_competition_with_deepseek(name, description=""):
    """ä½¿ç”¨DeepSeek AIåˆ†ææ¯”èµ›ç±»å‹å’Œéš¾åº¦ç­‰çº§"""
    if not deepseek_api_key:
        print("âš ï¸ DeepSeek APIå¯†é’¥æœªé…ç½®ï¼Œä½¿ç”¨é»˜è®¤åˆ†ç±»")
        return ['å…¶å®ƒ'], ['L2']
    
    try:
        # æ„å»ºåˆ†ææç¤º
        prompt = f"""
è¯·åˆ†æä»¥ä¸‹æ¯”èµ›ä¿¡æ¯ï¼Œå¹¶è¿”å›JSONæ ¼å¼çš„åˆ†ç±»ç»“æœï¼š

æ¯”èµ›åç§°ï¼š{name}
æ¯”èµ›æè¿°ï¼š{description}

è¯·æ ¹æ®ä»¥ä¸‹æ ‡å‡†è¿›è¡Œåˆ†ç±»ï¼š

æ¯”èµ›ç±»å‹ï¼ˆå¯å¤šé€‰ï¼‰ï¼š
- wibe coding: ç¼–ç¨‹ã€ç®—æ³•ã€ä»£ç ç›¸å…³æ¯”èµ›
- MCP: å¤šæ¨¡æ€ã€è·¨å¹³å°ã€ç»¼åˆæ€§æŠ€æœ¯æ¯”èµ›
- AIæ™ºèƒ½ä½“: AIã€æœºå™¨å­¦ä¹ ã€æ·±åº¦å­¦ä¹ ç›¸å…³æ¯”èµ›
- AIè§†é¢‘: è§†é¢‘å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€å¤šåª’ä½“ç›¸å…³æ¯”èµ›
- å…¶å®ƒ: ä¸å±äºä»¥ä¸Šç±»åˆ«çš„æ¯”èµ›

éš¾åº¦ç­‰çº§ï¼ˆå•é€‰ï¼‰ï¼š
- L1: åˆšæ¥è§¦ç”µè„‘ï¼Œé€‚åˆå®Œå…¨æ²¡æœ‰ç¼–ç¨‹åŸºç¡€çš„åˆå­¦è€…
- L2: ä¼šç”¨ç”µè„‘ï¼Œæœ‰åŸºæœ¬çš„è®¡ç®—æœºæ“ä½œèƒ½åŠ›
- L3: å¯¹ç”µè„‘æ¯”è¾ƒç†Ÿç»ƒï¼Œæœ‰ä¸€å®šçš„ç¼–ç¨‹å’Œå¼€å‘ç»éªŒ
- L4: å¯¹ç”µè„‘å¾ˆç†Ÿæ‚‰ï¼Œä¸“ä¸šå¼€å‘è€…æˆ–é«˜çº§æŠ€æœ¯äººå‘˜

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
  "competition_types": ["ç±»å‹1", "ç±»å‹2"],
  "difficulty_level": "L2"
}}
"""
        
        headers = {
            'Authorization': f'Bearer {deepseek_api_key}',
            'Content-Type': 'application/json'
        }
        
        data = {
            'model': 'deepseek-chat',
            'messages': [
                {
                    'role': 'user',
                    'content': prompt
                }
            ],
            'temperature': 0.1,
            'max_tokens': 500
        }
        
        response = requests.post(
            'https://api.deepseek.com/chat/completions',
            headers=headers,
            json=data,
            timeout=30
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            
            # å°è¯•è§£æJSONå“åº”
            try:
                # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                clean_content = content.strip()
                if clean_content.startswith('```json'):
                    clean_content = clean_content[7:]
                if clean_content.endswith('```'):
                    clean_content = clean_content[:-3]
                clean_content = clean_content.strip()
                
                analysis = json.loads(clean_content)
                comp_types = analysis.get('competition_types', ['å…¶å®ƒ'])
                difficulty = analysis.get('difficulty_level', 'L2')
                
                print(f"ğŸ¤– AIåˆ†æç»“æœ: ç±»å‹={comp_types}, éš¾åº¦={difficulty}")
                return comp_types, [difficulty]
            except json.JSONDecodeError:
                print(f"âš ï¸ AIå“åº”è§£æå¤±è´¥: {content}")
                return ['å…¶å®ƒ'], ['L2']
        else:
            print(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {response.status_code}")
            return ['å…¶å®ƒ'], ['L2']
            
    except Exception as e:
        print(f"âŒ DeepSeekåˆ†æå‡ºé”™: {str(e)}")
        return ['å…¶å®ƒ'], ['L2']

def get_feishu_token():
    # æ£€æŸ¥é…ç½®æ˜¯å¦å­˜åœ¨
    if not app_id or not app_secret:
        print("âŒ é£ä¹¦é…ç½®ç¼ºå¤±ï¼è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„FEISHU_APP_IDå’ŒFEISHU_APP_SECRET")
        print("ğŸ’¡ è¯·è®¿é—® https://open.feishu.cn/app åˆ›å»ºåº”ç”¨å¹¶è·å–é…ç½®ä¿¡æ¯")
        return None
        
    url = "https://open.feishu.cn/open-apis/auth/v3/tenant_access_token/internal/"
    payload = {"app_id": app_id, "app_secret": app_secret}
    
    try:
        response = requests.post(url, json=payload, timeout=30)
        if response.status_code != 200:
            print(f"âŒ è·å–é£ä¹¦tokenå¤±è´¥: {response.status_code} - {response.text}")
            return None
        
        data = response.json()
        if data.get('code') != 0:
            print(f"âŒ é£ä¹¦APIè¿”å›é”™è¯¯: {data.get('msg', 'æœªçŸ¥é”™è¯¯')}")
            return None
            
        return data["tenant_access_token"]
    except requests.exceptions.RequestException as e:
        print(f"âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"âŒ è§£æå“åº”å¤±è´¥: {e}")
        return None
def insert_to_feishu(name, link, start_date=None, end_date=None, cover_image="", difficulty_level="", competition_type="", description="", max_retries=3):
    """å°†æ¯”èµ›æ•°æ®æ’å…¥åˆ°é£ä¹¦è¡¨æ ¼ä¸­
    
    Args:
        name: æ¯”èµ›åç§°
        link: æ¯”èµ›é“¾æ¥
        start_date: æ¯”èµ›å¼€å§‹æ—¶é—´
        end_date: æ¯”èµ›ç»“æŸæ—¶é—´
        cover_image: æ¯”èµ›å°é¢
        difficulty_level: éš¾åº¦ç­‰çº§
        competition_type: æ¯”èµ›ç±»å‹
        description: æ¯”èµ›æè¿°ï¼ˆç”¨äºAIåˆ†æï¼‰
    
    Returns:
        bool: æ’å…¥æ˜¯å¦æˆåŠŸ
    """
    # æ•°æ®éªŒè¯
    if not name or not link:
        print(f"é”™è¯¯ï¼šæ¯”èµ›åç§°æˆ–é“¾æ¥ä¸ºç©º - åç§°: {name}, é“¾æ¥: {link}")
        return False
    
    # æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
    exists, record_id = check_record_exists(name, link)
    if exists:
        # å¦‚æœè®°å½•å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¹¶æ›´æ–°çŠ¶æ€
        competition_status = determine_competition_status(start_date, end_date)
        if competition_status == "å·²ç»“æŸ":
            if update_record_status(record_id, "å·²ç»“æŸ"):
                print(f"âœ… å·²æ›´æ–°æ¯”èµ›çŠ¶æ€ä¸ºå·²ç»“æŸ: {name}")
            else:
                print(f"âŒ æ›´æ–°æ¯”èµ›çŠ¶æ€å¤±è´¥: {name}")
        print(f"â­ï¸ è·³è¿‡é‡å¤è®°å½•: {name}")
        return True
    
    # ä½¿ç”¨DeepSeek AIåˆ†ææ¯”èµ›ç±»å‹å’Œéš¾åº¦ç­‰çº§ï¼ˆå¦‚æœæœªæä¾›æˆ–ä¸ºç©ºï¼‰
    if not difficulty_level or not competition_type:
        print(f"ğŸ¤– ä½¿ç”¨AIåˆ†ææ¯”èµ›: {name}")
        ai_types, ai_difficulty = analyze_competition_with_deepseek(name, description)
        
        # å¦‚æœåŸå‚æ•°ä¸ºç©ºï¼Œä½¿ç”¨AIåˆ†æç»“æœ
        if not competition_type:
            competition_type = ai_types[0] if ai_types else "å…¶å®ƒ"
        if not difficulty_level:
            difficulty_level = ai_difficulty[0] if ai_difficulty else "L2"
    
    # ç¡®å®šæ¯”èµ›çŠ¶æ€
    competition_status = determine_competition_status(start_date, end_date)
    
    # è·å–è®¿é—®ä»¤ç‰Œ
    token = get_feishu_token()
    if not token:
        print("é”™è¯¯ï¼šæ— æ³•è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ")
        return False
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    
    # æ„å»ºè¯·æ±‚æ•°æ® - æŒ‰ç…§é£ä¹¦è¦æ±‚çš„æ ¼å¼
    def clean_text(text):
        """æ¸…ç†æ–‡æœ¬ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œæ¢è¡Œç¬¦"""
        if not text:
            return ""
        # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶ç§»é™¤æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ç‰¹æ®Šå­—ç¬¦
        cleaned = str(text).replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        cleaned = ' '.join(cleaned.split())
        return cleaned
    
    # æ ¹æ®é£ä¹¦è¡¨æ ¼å­—æ®µæ ¼å¼è¦æ±‚æ„å»ºæ•°æ®
    def format_difficulty_level(level):
        """å°†éš¾åº¦ç­‰çº§è½¬æ¢ä¸ºå¤šé€‰æ ¼å¼"""
        if not level:
            return []
        level_str = str(level).upper()
        valid_levels = ['L1', 'L2', 'L3', 'L4']
        # å°è¯•åŒ¹é…å·²çŸ¥çš„éš¾åº¦ç­‰çº§
        for valid_level in valid_levels:
            if valid_level in level_str:
                return [valid_level]
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ï¼Œé»˜è®¤è¿”å›L2
        return ['L2']
    
    def format_competition_type(comp_type):
        """å°†æ¯”èµ›ç±»å‹è½¬æ¢ä¸ºå¤šé€‰æ ¼å¼"""
        if not comp_type:
            return []
        type_str = str(comp_type).lower()
        valid_types = ['wibe coding', 'MCP', 'AIæ™ºèƒ½ä½“', 'AIè§†é¢‘', 'å…¶å®ƒ']
        result = []
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å·²çŸ¥ç±»å‹
        if 'coding' in type_str or 'ç¼–ç¨‹' in type_str:
            result.append('wibe coding')
        elif 'mcp' in type_str:
            result.append('MCP')
        elif 'ai' in type_str or 'æ™ºèƒ½' in type_str:
            if 'è§†é¢‘' in type_str or 'video' in type_str:
                result.append('AIè§†é¢‘')
            else:
                result.append('AIæ™ºèƒ½ä½“')
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ç±»å‹ï¼Œè¿”å›å…¶å®ƒ
        return result if result else ['å…¶å®ƒ']
    
    payload = {
        "fields": {
            "æ¯”èµ›åç§°": clean_text(name)[:500],
            "æ¯”èµ›é“¾æ¥": {
                "text": clean_text(name)[:100],
                "link": clean_text(link)[:500]
            },
            "æ¯”èµ›çŠ¶æ€": competition_status,
            "éš¾åº¦ç­‰çº§": format_difficulty_level(difficulty_level),
            "æ¯”èµ›ç±»å‹": format_competition_type(competition_type)
        }
    }
    
    # å¦‚æœæœ‰å°é¢å›¾ç‰‡URLï¼ŒæŒ‰ç…§é£ä¹¦é™„ä»¶æ ¼å¼æ·»åŠ 
    if cover_image and cover_image.strip():
        payload["fields"]["æ¯”èµ›å°é¢"] = [{
            "file_token": "",
            "name": "æ¯”èµ›å°é¢",
            "url": clean_text(cover_image)[:500]
        }]
    
    # é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            response = requests.post(url, headers=headers, json=payload, timeout=30)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    print(f"âœ… æˆåŠŸæ’å…¥æ•°æ®: {name}")
                    return True
                else:
                    print(f"âŒ é£ä¹¦APIè¿”å›é”™è¯¯: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                    print(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {result}")
            elif response.status_code == 401:
                print("âŒ è®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é£ä¹¦åº”ç”¨é…ç½®")
                return False
            elif response.status_code == 429:
                print(f"âš ï¸ è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œç­‰å¾…åé‡è¯• (å°è¯• {attempt + 1}/{max_retries})")
                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                continue
            else:
                print(f"âŒ HTTPé”™è¯¯ {response.status_code}: {response.text}")
                
        except requests.exceptions.Timeout:
            print(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼Œé‡è¯•ä¸­ (å°è¯• {attempt + 1}/{max_retries})")
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ ç½‘ç»œé”™è¯¯: {e} (å°è¯• {attempt + 1}/{max_retries})")
        except Exception as e:
            print(f"âŒ æœªçŸ¥é”™è¯¯: {e}")
            return False
        
        if attempt < max_retries - 1:
            time.sleep(1)  # é‡è¯•å‰ç­‰å¾…
    
    print(f"âŒ æ’å…¥å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡: {name}")
    return False

def get_all_records():
    """è·å–è¡¨æ ¼ä¸­çš„æ‰€æœ‰è®°å½•
    
    Returns:
        list: æ‰€æœ‰è®°å½•çš„åˆ—è¡¨ï¼Œæ¯ä¸ªè®°å½•åŒ…å«record_idå’Œfields
    """
    token = get_feishu_token()
    if not token:
        return []
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    
    all_records = []
    page_token = None
    
    try:
        while True:
            params = {"page_size": 500}
            if page_token:
                params["page_token"] = page_token
            
            response = requests.get(url, headers=headers, params=params, timeout=30)
            if response.status_code == 200:
                result = response.json()
                if result.get('code') == 0:
                    data = result.get('data', {})
                    records = data.get('items', [])
                    all_records.extend(records)
                    
                    # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰æ›´å¤šé¡µé¢
                    page_token = data.get('page_token')
                    if not page_token:
                        break
                else:
                    print(f"âŒ è·å–è®°å½•å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
                    break
            else:
                print(f"âŒ HTTPé”™è¯¯ {response.status_code}: {response.text}")
                break
                
    except Exception as e:
        print(f"âš ï¸ è·å–æ‰€æœ‰è®°å½•æ—¶å‡ºé”™: {e}")
    
    return all_records

def check_record_exists(name, link):
    """æ£€æŸ¥è®°å½•æ˜¯å¦å·²å­˜åœ¨
    
    Args:
        name: æ¯”èµ›åç§°
        link: æ¯”èµ›é“¾æ¥
    
    Returns:
        tuple: (æ˜¯å¦å­˜åœ¨, è®°å½•ID) - å¦‚æœå­˜åœ¨è¿”å›(True, record_id)ï¼Œå¦åˆ™è¿”å›(False, None)
    """
    token = get_feishu_token()
    if not token:
        return False, None
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/search"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    
    # ä½¿ç”¨æ¯”èµ›é“¾æ¥ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦è¿›è¡ŒæŸ¥è¯¢
    payload = {
        "filter": {
            "conditions": [
                {
                    "field_name": "æ¯”èµ›é“¾æ¥",
                    "operator": "is",
                    "value": [link]
                }
            ],
            "conjunction": "and"
        }
    }
    
    try:
        response = requests.post(url, headers=headers, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            if result.get('code') == 0:
                records = result.get('data', {}).get('items', [])
                if len(records) > 0:
                    return True, records[0].get('record_id')
                return False, None
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥è®°å½•å­˜åœ¨æ€§æ—¶å‡ºé”™: {e}")
    
    return False, None

def update_record_status(record_id, status):
    """æ›´æ–°è®°å½•çš„æ¯”èµ›çŠ¶æ€
    
    Args:
        record_id: è®°å½•ID
        status: æ¯”èµ›çŠ¶æ€ ("è¿›è¡Œä¸­" æˆ– "å·²ç»“æŸ")
    
    Returns:
        bool: æ›´æ–°æ˜¯å¦æˆåŠŸ
    """
    token = get_feishu_token()
    if not token:
        return False
    
    url = f"https://open.feishu.cn/open-apis/bitable/v1/apps/{app_token}/tables/{table_id}/records/{record_id}"
    headers = {"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    
    payload = {
        "fields": {
            "æ¯”èµ›çŠ¶æ€": status
        }
    }
    
    try:
        response = requests.put(url, headers=headers, json=payload, timeout=30)
        if response.status_code == 200:
            result = response.json()
            if result.get('code') == 0:
                return True
            else:
                print(f"âŒ æ›´æ–°è®°å½•çŠ¶æ€å¤±è´¥: {result.get('msg', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print(f"âŒ HTTPé”™è¯¯ {response.status_code}: {response.text}")
    except Exception as e:
        print(f"âš ï¸ æ›´æ–°è®°å½•çŠ¶æ€æ—¶å‡ºé”™: {e}")
    
    return False

def determine_competition_status(start_date, end_date):
    """æ ¹æ®æ¯”èµ›æ—¶é—´ç¡®å®šæ¯”èµ›çŠ¶æ€
    
    Args:
        start_date: å¼€å§‹æ—¶é—´
        end_date: ç»“æŸæ—¶é—´
    
    Returns:
        str: æ¯”èµ›çŠ¶æ€ ("è¿›è¡Œä¸­" æˆ– "å·²ç»“æŸ")
    """
    now = datetime.datetime.now()
    
    # å¦‚æœæœ‰ç»“æŸæ—¶é—´ä¸”å·²è¿‡æœŸï¼Œåˆ™ä¸ºå·²ç»“æŸ
    if end_date and end_date < now:
        return "å·²ç»“æŸ"
    
    # å¦‚æœæœ‰å¼€å§‹æ—¶é—´ä¸”è¿˜æœªå¼€å§‹ï¼Œåˆ™ä¸ºè¿›è¡Œä¸­ï¼ˆå³å°†å¼€å§‹ï¼‰
    # å¦‚æœæ­£åœ¨è¿›è¡Œä¸­æˆ–æ—¶é—´ä¸æ˜ç¡®ï¼Œé»˜è®¤ä¸ºè¿›è¡Œä¸­
    return "è¿›è¡Œä¸­"

def batch_insert_to_feishu(records_data, batch_size=10):
    """æ‰¹é‡æ’å…¥æ•°æ®åˆ°é£ä¹¦è¡¨æ ¼
    
    Args:
        records_data: è®°å½•æ•°æ®åˆ—è¡¨ï¼Œæ¯æ¡è®°å½•åŒ…å«(name, link, competition_time, cover_image, difficulty_level, competition_type)
        batch_size: æ‰¹é‡å¤§å°
    
    Returns:
        tuple: (æˆåŠŸæ•°é‡, å¤±è´¥æ•°é‡)
    """
    # é¦–å…ˆæ£€æŸ¥é£ä¹¦é…ç½®
    if not app_id or not app_secret:
        print("âŒ æ— æ³•æ’å…¥æ•°æ®ï¼šé£ä¹¦é…ç½®ç¼ºå¤±")
        print("ğŸ’¡ è¯·åœ¨.envæ–‡ä»¶ä¸­é…ç½®FEISHU_APP_IDå’ŒFEISHU_APP_SECRET")
        print("ğŸ“ æ•°æ®å·²æ”¶é›†å®Œæˆï¼Œé…ç½®å¥½é£ä¹¦åå¯é‡æ–°è¿è¡Œç¨‹åºæ’å…¥æ•°æ®")
        return 0, len(records_data)
    
    success_count = 0
    failed_count = 0
    
    # æµ‹è¯•é£ä¹¦è¿æ¥
    test_token = get_feishu_token()
    if not test_token:
        print("âŒ æ— æ³•è¿æ¥åˆ°é£ä¹¦ï¼Œè·³è¿‡æ•°æ®æ’å…¥")
        return 0, len(records_data)
    
    print(f"âœ… é£ä¹¦è¿æ¥æ­£å¸¸ï¼Œå¼€å§‹æ‰¹é‡æ’å…¥ {len(records_data)} æ¡æ•°æ®...")
    
    for i in range(0, len(records_data), batch_size):
        batch = records_data[i:i + batch_size]
        print(f"\nğŸ“¦ å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}: {len(batch)} æ¡è®°å½•")
        
        for record in batch:
            name, link, competition_time, cover_image, difficulty_level, competition_type = record
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆè¿™é‡Œçš„é€»è¾‘å·²ç»åœ¨insert_to_feishuä¸­å¤„ç†äº†ï¼‰
            # æ’å…¥è®°å½•
            if insert_to_feishu(name, link, None, None, cover_image, difficulty_level, competition_type):
                success_count += 1
            else:
                failed_count += 1
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
            time.sleep(0.5)
        
        # æ‰¹æ¬¡é—´ç¨é•¿å»¶è¿Ÿ
        if i + batch_size < len(records_data):
            time.sleep(2)
    
    print(f"\nğŸ“Š æ‰¹é‡æ’å…¥å®Œæˆ: æˆåŠŸ {success_count} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")
    return success_count, failed_count

def crawl_wechat(biz, token, cookie):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Cookie': cookie
    }
    # Fetch biz banner first
    print("Fetching biz banner...")
    banner = get_biz_banner(biz, token, cookie)
    print("Biz Banner:", banner)
    base_url = 'https://mp.weixin.qq.com/cgi-bin/appmsg'
    begin = 0
    while True:
        params = {
            'action': 'list_ex',
            'begin': str(begin),
            'count': '5',
            'fakeid': biz,
            'type': '9',
            'query': '',
            'token': token,
            'lang': 'zh_CN',
            'f': 'json',
            'ajax': '1'
        }
        response = requests.get(base_url, headers=headers, params=params)
        data = response.json()
        if 'app_msg_list' not in data or not data['app_msg_list']:
            break
        for item in data['app_msg_list']:
            if 'èµ›' in item['title']:
                link = item['link']
                participants, prize, start_date, end_date = get_details(link)  # Reuse get_details for article
                if (start_date is None or start_date <= datetime.datetime.now()) and (end_date is None or end_date >= datetime.datetime.now()):
                    print(f"Title: {item['title']}")
                    print(f"Link: {item['link']}")
                    print(f"Create Time: {item['create_time']}")
                    print(f"Participants: {participants}")
                    print(f"Prize: {prize}")
                    # Parse mid and idx from link
                    from urllib.parse import urlparse, parse_qs
                    parsed_url = urlparse(item['link'])
                    query_params = parse_qs(parsed_url.query)
                    mid = query_params.get('mid', [''])[0]
                    idx = query_params.get('idx', [''])[0]
                    sn = query_params.get('sn', [''])[0]
                    if mid and idx:
                        print("Fetching comments...")
                        comments = get_comments(biz, token, cookie, mid, idx)
                        for comment in comments:
                            print(f"Comment: {comment['content']}")
                            print(f"Author: {comment.get('nick_name', 'Anonymous')}")
                            print("---")
                        print("Fetching video snaps...")
                        video_snaps = get_video_snaps(biz, token, cookie)
                    print("---")
        begin += 5
        time.sleep(2)

def get_biz_banner(biz, token, cookie):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Cookie': cookie
    }
    base_url = 'https://mp.weixin.qq.com/mp/getbizbanner'
    params = {
        '__biz': biz,
        'uin': '',
        'key': '',
        'pass_ticket': '',
        'wxtoken': '777',
        'devicetype': '',
        'clientversion': 'false',
        'version': 'false',
        'appmsg_token': '',
        'x5': '0',
        'f': 'json',
        'user_article_role': '0'
    }
    response = requests.get(base_url, headers=headers, params=params)
    data = response.json()
    return data

def get_video_snaps(biz, appmsg_token, cookie):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Cookie': cookie
    }
    base_url = 'https://mp.weixin.qq.com/mp/appmsg_video_snap'
    params = {
        'action': 'batch_get_video_snap',
        'uin': '',
        'key': '',
        'pass_ticket': '',
        'wxtoken': '777',
        'devicetype': '',
        'clientversion': 'false',
        'version': 'false',
        '__biz': biz,
        'appmsg_token': appmsg_token,
        'x5': '0',
        'f': 'json',
        'user_article_role': '0'
    }
    response = requests.get(base_url, headers=headers, params=params)
    data = response.json()
    print("Video Snaps:", data)
    return data
def get_comments(biz, token, cookie, appmsgid, idx):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3',
        'Cookie': cookie
    }
    base_url = 'https://mp.weixin.qq.com/mp/appmsg_comment'
    params = {
        'action': 'getcomment',
        'scene': '0',
        '__biz': biz,
        'appmsgid': appmsgid,
        'idx': idx,
        'comment_id': '',  # Can be set if needed for specific comment
        'offset': '0',
        'limit': '100',
        'send_time': '',
        'sessionid': '',
        'enterid': '',
        'top_content_id': '',
        'top_reply_id': '',
        'comment_scene': '0',
        'uin': '',
        'key': '',
        'pass_ticket': '',
        'wxtoken': '777',
        'devicetype': '',
        'clientversion': 'false',
        'version': 'false',
        'appmsg_token': '',
        'x5': '0',
        'f': 'json',
        'user_article_role': '0'
    }
    response = requests.get(base_url, headers=headers, params=params)
    data = response.json()
    if 'base_resp' in data and data['base_resp']['ret'] == 0:
        comments = data.get('elected_comment', [])
        for comment in comments:
            print(f"Comment ID: {comment['comment_id']}")
            print(f"Content: {comment['content']}")
            if 'reply_list' in comment:
                for reply in comment['reply_list']:
                    print(f"Reply: {reply['content']}")
        # Parse cookie to extract parameters
        cookie_dict = dict(item.split('=') for item in cookie.split('; ') if '=' in item)
        uin = cookie_dict.get('wxuin', '')
        key = cookie_dict.get('rand_info', '')  # Assuming rand_info might be key, adjust if needed
        pass_ticket = cookie_dict.get('data_ticket', '')  # Assuming data_ticket as pass_ticket, adjust if needed
        # Fetch identity list
        identity_params = {
            'action': 'getidentitylist',
            'scene': '0',
            'register': '0',
            'uin': uin,
            'key': key,
            'pass_ticket': pass_ticket,
            'wxtoken': '777',
            'devicetype': '',
            'clientversion': 'false',
            'version': 'false',
            '__biz': biz,
            'appmsg_token': token,  # Use provided token as appmsg_token
            'x5': '0',
            'f': 'json',
            'user_article_role': '0'
        }
        identity_response = requests.get(base_url, headers=headers, params=identity_params)
        identity_data = identity_response.json()
        print("Identity List:", identity_data)
        return comments
    else:
        print("Failed to fetch comments:", data)
        return []

def crawl_baidu():
    """çˆ¬å–ç™¾åº¦AI Studioæ¯”èµ›ä¿¡æ¯"""
    print("ğŸš€ å¼€å§‹çˆ¬å–ç™¾åº¦AI Studioæ¯”èµ›ä¿¡æ¯...")
    base_url = "https://aistudio.baidu.com/studio/match/search?pageSize=10&matchType=0&matchStatus=1&keyword=&orderBy=0"
    page = 1
    success_count = 0
    failed_count = 0
    
    # æ£€æŸ¥é£ä¹¦é…ç½®
    if not app_id or not app_secret:
        print("âŒ é£ä¹¦é…ç½®ç¼ºå¤±ï¼è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„FEISHU_APP_IDå’ŒFEISHU_APP_SECRET")
        print("ğŸ’¡ è¯·è®¿é—® https://open.feishu.cn/app åˆ›å»ºåº”ç”¨å¹¶è·å–é…ç½®ä¿¡æ¯")
        return
    
    # æµ‹è¯•é£ä¹¦è¿æ¥
    test_token = get_feishu_token()
    if not test_token:
        print("âŒ æ— æ³•è¿æ¥åˆ°é£ä¹¦ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    print("âœ… é£ä¹¦è¿æ¥æ­£å¸¸ï¼Œå¼€å§‹é€ä¸ªå¯¼å…¥æ¯”èµ›æ•°æ®...")
    
    while True:
        url = f"{base_url}&p={page}"
        try:
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            data = response.json()
            items = data['result']['data']
            
            if not items:
                break
                
            print(f"ğŸ“„ å¤„ç†ç¬¬ {page} é¡µï¼Œæ‰¾åˆ° {len(items)} ä¸ªæ¯”èµ›")
            
            for item in items:
                name = item['matchName']
                intro = item['matchAbs']
                link = f"https://aistudio.baidu.com/studio/match/detail/{item['id']}"
                
                participants, prize, start_date, end_date = get_details(link)

                # åªå¤„ç†æœ‰æ•ˆçš„æ¯”èµ›æ•°æ®ï¼ˆè¿›è¡Œä¸­æˆ–å³å°†å¼€å§‹çš„æ¯”èµ›ï¼‰
                if (start_date is None or start_date <= datetime.datetime.now()) and (end_date is None or end_date >= datetime.datetime.now()):
                    print(f"âœ… æ”¶é›†æ¯”èµ›: {name}")
                    
                    # ç«‹å³æ’å…¥åˆ°é£ä¹¦è¡¨æ ¼ï¼Œä¼ é€’ä»‹ç»ä¿¡æ¯ç”¨äºAIåˆ†æ
                    if insert_to_feishu(name, link, start_date, end_date, "", "", "", intro):
                        success_count += 1
                        print(f"ğŸ“ æˆåŠŸå¯¼å…¥ç¬¬ {success_count} æ¡æ•°æ®")
                    else:
                        failed_count += 1
                        print(f"âŒ å¯¼å…¥å¤±è´¥: {name}")
                    
                    # æ¯æ¬¡æ’å…¥åç¨ä½œå»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(1)
                else:
                    print(f"â­ï¸ è·³è¿‡è¿‡æœŸæ¯”èµ›: {name}")
                    
            page += 1
            time.sleep(2)  # é¡µé¢é—´å»¶è¿Ÿ
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
            break
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
            break
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç™¾åº¦AI Studioçˆ¬å–å®Œæˆ: æˆåŠŸå¯¼å…¥ {success_count} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")

def crawl_aliyun():
    """çˆ¬å–é˜¿é‡Œå¤©æ± æ¯”èµ›ä¿¡æ¯"""
    print("ğŸš€ å¼€å§‹çˆ¬å–é˜¿é‡Œå¤©æ± æ¯”èµ›ä¿¡æ¯...")
    base_url = "https://tianchi.aliyun.com/v3/proxy/competition/api/race/page?visualTab=&raceName=&isActive="
    page = 1
    success_count = 0
    failed_count = 0
    
    # æ£€æŸ¥é£ä¹¦é…ç½®
    if not app_id or not app_secret:
        print("âŒ é£ä¹¦é…ç½®ç¼ºå¤±ï¼è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„FEISHU_APP_IDå’ŒFEISHU_APP_SECRET")
        print("ğŸ’¡ è¯·è®¿é—® https://open.feishu.cn/app åˆ›å»ºåº”ç”¨å¹¶è·å–é…ç½®ä¿¡æ¯")
        return
    
    # æµ‹è¯•é£ä¹¦è¿æ¥
    test_token = get_feishu_token()
    if not test_token:
        print("âŒ æ— æ³•è¿æ¥åˆ°é£ä¹¦ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    print("âœ… é£ä¹¦è¿æ¥æ­£å¸¸ï¼Œå¼€å§‹é€ä¸ªå¯¼å…¥æ¯”èµ›æ•°æ®...")
    
    while True:
        url = f"{base_url}&pageNum={page}"
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
        
        try:
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            
            if not data.get('success', False):
                print(f"âŒ APIè¿”å›å¤±è´¥: {data.get('message', 'æœªçŸ¥é”™è¯¯')}")
                break
                
            items = data.get('data', {}).get('list', [])
            if not items:
                break
                
            print(f"ğŸ“„ å¤„ç†ç¬¬ {page} é¡µï¼Œæ‰¾åˆ° {len(items)} ä¸ªæ¯”èµ›")
            
            for item in items:
                name = item.get('name', 'Unknown')
                intro = item.get('introduction', 'Not available')
                link = f"https://tianchi.aliyun.com/competition/entrance/{item.get('raceId', '')}/introduction"
                
                # å°è¯•ä»APIæ•°æ®ä¸­è·å–æ—¥æœŸä¿¡æ¯
                api_start_date = None
                api_end_date = None
                
                if 'gmtStart' in item and item['gmtStart']:
                    try:
                        api_start_date = datetime.datetime.fromtimestamp(item['gmtStart'] / 1000)
                    except:
                        pass
                        
                if 'gmtEnd' in item and item['gmtEnd']:
                    try:
                        api_end_date = datetime.datetime.fromtimestamp(item['gmtEnd'] / 1000)
                    except:
                        pass
                
                # ä»é¡µé¢è·å–è¯¦ç»†ä¿¡æ¯
                participants, prize, page_start_date, page_end_date = get_details(link)
                
                # ä¼˜å…ˆä½¿ç”¨APIä¸­çš„æ—¥æœŸï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é¡µé¢æå–çš„æ—¥æœŸ
                start_date = api_start_date or page_start_date
                end_date = api_end_date or page_end_date
                
                # åªå¤„ç†æœ‰æ•ˆçš„æ¯”èµ›æ•°æ®ï¼ˆè¿›è¡Œä¸­æˆ–å³å°†å¼€å§‹çš„æ¯”èµ›ï¼‰
                if (start_date is None or start_date <= datetime.datetime.now()) and (end_date is None or end_date >= datetime.datetime.now()):
                    print(f"âœ… æ”¶é›†æ¯”èµ›: {name}")
                    
                    # ç«‹å³æ’å…¥åˆ°é£ä¹¦è¡¨æ ¼ï¼Œä¼ é€’ä»‹ç»ä¿¡æ¯ç”¨äºAIåˆ†æ
                    if insert_to_feishu(name, link, start_date, end_date, "", "", "", intro):
                        success_count += 1
                        print(f"ğŸ“ æˆåŠŸå¯¼å…¥ç¬¬ {success_count} æ¡æ•°æ®")
                    else:
                        failed_count += 1
                        print(f"âŒ å¯¼å…¥å¤±è´¥: {name}")
                    
                    # æ¯æ¬¡æ’å…¥åç¨ä½œå»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                    time.sleep(1)
                else:
                    print(f"â­ï¸ è·³è¿‡è¿‡æœŸæ¯”èµ›: {name}")
                    
            page += 1
            time.sleep(2)  # é¡µé¢é—´å»¶è¿Ÿ
            
        except requests.exceptions.RequestException as e:
            print(f"âŒ è¯·æ±‚ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
            break
        except Exception as e:
            print(f"âŒ å¤„ç†ç¬¬ {page} é¡µæ—¶å‡ºé”™: {e}")
            break
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š é˜¿é‡Œå¤©æ± çˆ¬å–å®Œæˆ: æˆåŠŸå¯¼å…¥ {success_count} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")

def crawl_tencent():
    """çˆ¬å–è…¾è®¯CSDNåšå®¢æ¯”èµ›ä¿¡æ¯"""
    print("ğŸš€ å¼€å§‹çˆ¬å–è…¾è®¯CSDNåšå®¢æ¯”èµ›ä¿¡æ¯...")
    success_count = 0
    failed_count = 0
    
    # æ£€æŸ¥é£ä¹¦é…ç½®
    if not app_id or not app_secret:
        print("âŒ é£ä¹¦é…ç½®ç¼ºå¤±ï¼è¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„FEISHU_APP_IDå’ŒFEISHU_APP_SECRET")
        print("ğŸ’¡ è¯·è®¿é—® https://open.feishu.cn/app åˆ›å»ºåº”ç”¨å¹¶è·å–é…ç½®ä¿¡æ¯")
        return
    
    # æµ‹è¯•é£ä¹¦è¿æ¥
    test_token = get_feishu_token()
    if not test_token:
        print("âŒ æ— æ³•è¿æ¥åˆ°é£ä¹¦ï¼Œè¯·æ£€æŸ¥é…ç½®")
        return
    
    print("âœ… é£ä¹¦è¿æ¥æ­£å¸¸ï¼Œå¼€å§‹é€ä¸ªå¯¼å…¥æ¯”èµ›æ•°æ®...")
    
    try:
        from selenium import webdriver
        from selenium.webdriver.chrome.options import Options
        from selenium.webdriver.common.by import By
        from selenium.webdriver.support.ui import WebDriverWait
        from selenium.webdriver.support import expected_conditions as EC
        
        options = Options()
        options.headless = True
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        
        driver = webdriver.Chrome(options=options)
        url = 'https://blog.csdn.net/QcloudCommunity/article/list'
        
        print(f"ğŸ“„ æ­£åœ¨è®¿é—®: {url}")
        driver.get(url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )
        
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        driver.quit()
        
        # æŸ¥æ‰¾æ–‡ç« 
        articles = soup.find_all('div', class_='article-item-box')
        if not articles:
            # å°è¯•å…¶ä»–å¯èƒ½çš„é€‰æ‹©å™¨
            articles = soup.find_all('article') or soup.find_all('div', class_='article')
        
        print(f"ğŸ“„ æ‰¾åˆ°çš„æ–‡ç« æ•°é‡: {len(articles)}")
        
        for article in articles:
            try:
                # å°è¯•å¤šç§æ ‡é¢˜é€‰æ‹©å™¨
                title_elem = (article.find('h4', class_='text-truncate') or 
                             article.find('h3') or 
                             article.find('h2') or 
                             article.find('a', class_='title'))
                
                if title_elem and 'èµ›' in title_elem.get_text():
                    name = title_elem.get_text().strip()
                    
                    # è·å–é“¾æ¥
                    link_elem = title_elem.find('a') if title_elem.name != 'a' else title_elem
                    if not link_elem:
                        continue
                        
                    link = link_elem.get('href', '')
                    if link and not link.startswith('http'):
                        link = f"https://blog.csdn.net{link}"
                    
                    # è·å–ä»‹ç»
                    intro_elem = article.find('p', class_='content') or article.find('div', class_='summary')
                    intro = intro_elem.get_text().strip() if intro_elem else 'Not found'
                    
                    # è·å–è¯¦ç»†ä¿¡æ¯
                    participants, prize, start_date, end_date = get_details(link)
                    
                    # åªå¤„ç†æœ‰æ•ˆçš„æ¯”èµ›æ•°æ®ï¼ˆè¿›è¡Œä¸­æˆ–å³å°†å¼€å§‹çš„æ¯”èµ›ï¼‰
                    if (start_date is None or start_date <= datetime.datetime.now()) and (end_date is None or end_date >= datetime.datetime.now()):
                        print(f"âœ… æ”¶é›†æ¯”èµ›: {name}")
                        
                        # ç«‹å³æ’å…¥åˆ°é£ä¹¦è¡¨æ ¼ï¼Œä¼ é€’ä»‹ç»ä¿¡æ¯ç”¨äºAIåˆ†æ
                        if insert_to_feishu(name, link, start_date, end_date, "", "", "", intro):
                            success_count += 1
                            print(f"ğŸ“ æˆåŠŸå¯¼å…¥ç¬¬ {success_count} æ¡æ•°æ®")
                        else:
                            failed_count += 1
                            print(f"âŒ å¯¼å…¥å¤±è´¥: {name}")
                        
                        # æ¯æ¬¡æ’å…¥åç¨ä½œå»¶è¿Ÿï¼Œé¿å…é¢‘ç‡é™åˆ¶
                        time.sleep(1)
                    else:
                        print(f"â­ï¸ è·³è¿‡è¿‡æœŸæ¯”èµ›: {name}")
                        
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                continue
                
    except ImportError:
        print("âŒ ç¼ºå°‘seleniumä¾èµ–ï¼Œè¯·å®‰è£…: pip install selenium")
        return
    except Exception as e:
        print(f"âŒ çˆ¬å–è…¾è®¯åšå®¢æ—¶å‡ºé”™: {e}")
        return
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š è…¾è®¯CSDNåšå®¢çˆ¬å–å®Œæˆ: æˆåŠŸå¯¼å…¥ {success_count} æ¡ï¼Œå¤±è´¥ {failed_count} æ¡")

import datetime
def get_details(link):
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.webdriver.chrome.options import Options
    import time
    options = Options()
    options.headless = True
    service = Service(ChromeDriverManager().install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(link)
    time.sleep(2)  # æ·»åŠ å»¶è¿Ÿç¡®ä¿é¡µé¢åŠ è½½
    page_text = driver.find_element(By.TAG_NAME, 'body').text
    print("å®Œæ•´é¡µé¢æ–‡æœ¬:", page_text)
    participants = 'Not found'
    prize = 'Not found'
    start_date = None
    end_date = None
    participants_match = re.search(r"æŠ¥åäººæ•°[:ï¼š]?\s*(\d+)", page_text)
    if participants_match:
        participants = participants_match.group(1)
    prize_match = re.search(r"(å¥–æ± |å¥–é‡‘)[:ï¼š]?\s*([\w\dÂ¥,]+)", page_text)
    if prize_match:
        prize = prize_match.group(2)
    # Improved date regex to support more formats, including missing year and time
    date_pattern = r'(\d{4}[å¹´.-/]?\d{1,2}[æœˆ.-/]?\d{1,2}[æ—¥]?(?:\s*\d{1,2}[ç‚¹:]\d{2}(?::\d{2})?)?|å³æ—¥èµ·|\d{1,2}[æœˆ.-/]?\d{1,2}[æ—¥]?(?:\s*\d{1,2}[ç‚¹:]\d{2}(?::\d{2})?)?)'
    all_dates = re.findall(date_pattern, page_text)
    print("é¡µé¢ä¸­æ‰€æœ‰æ½œåœ¨æ—¥æœŸå­—ç¬¦ä¸²:", all_dates)
    start_date_match = re.search(r"(å¼€å§‹æ—¥æœŸ|èµ·å§‹æ—¥æœŸ|å¼€å§‹æ—¶é—´|æ¯”èµ›æ—¶é—´|æ´»åŠ¨æ—¶é—´|æŠ¥åæ—¶é—´)[:ï¼š]?\s*" + date_pattern, page_text)
    if start_date_match:
        print("åŒ¹é…åˆ°çš„å¼€å§‹æ—¥æœŸå­—ç¬¦ä¸²:", start_date_match.group(0))
        date_str = start_date_match.group(1).strip()
        if date_str == 'å³æ—¥èµ·':
            start_date = datetime.datetime.now()
        else:
            date_str = date_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('.', '-').replace('/', '-').replace('ç‚¹', ':')
            formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d', '%m-%d %H:%M:%S', '%m-%d %H:%M', '%m-%d']
            for fmt in formats:
                try:
                    if '%Y' not in fmt:
                        date_str = f"{datetime.datetime.now().year}-{date_str}"
                    start_date = datetime.datetime.strptime(date_str, fmt)
                    break
                except ValueError:
                    pass
    end_date_match = re.search(r"(ç»“æŸæ—¥æœŸ|æˆªæ­¢æ—¥æœŸ|æˆªæ­¢æ—¶é—´|æ¯”èµ›æ—¶é—´|æ´»åŠ¨æ—¶é—´|æŠ¥åæ—¶é—´)[:ï¼š]?\s*" + date_pattern, page_text)
    if end_date_match:
        print("åŒ¹é…åˆ°çš„ç»“æŸæ—¥æœŸå­—ç¬¦ä¸²:", end_date_match.group(0))
        date_str = end_date_match.group(1).strip()
        date_str = date_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('.', '-').replace('/', '-').replace('ç‚¹', ':')
        formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d', '%m-%d %H:%M:%S', '%m-%d %H:%M', '%m-%d']
        for fmt in formats:
            try:
                if '%Y' not in fmt:
                    date_str = f"{datetime.datetime.now().year}-{date_str}"
                end_date = datetime.datetime.strptime(date_str, fmt)
                break
            except ValueError:
                pass
    # Handle range like 'å³æ—¥èµ·-9æœˆ23æ—¥23ç‚¹59åˆ†59ç§’'
    range_match = re.search(r"(å¼€å§‹æ—¥æœŸ|èµ·å§‹æ—¥æœŸ|æ¯”èµ›æ—¶é—´|æ´»åŠ¨æ—¶é—´|æŠ¥åæ—¶é—´)[:ï¼š]?\s*(å³æ—¥èµ·|\d{4}[å¹´.-/]?\d{1,2}[æœˆ.-/]?\d{1,2}[æ—¥]?)\s*(?:-|è‡³|åˆ°)\s*(\d{4}[å¹´.-/]?\d{1,2}[æœˆ.-/]?\d{1,2}[æ—¥]?(?:\s*\d{1,2}[ç‚¹:]\d{2}(?::\d{2})?)?)", page_text)
    if range_match:
        print("åŒ¹é…åˆ°çš„æ—¥æœŸèŒƒå›´å­—ç¬¦ä¸²:", range_match.group(0))
        start_str = range_match.group(2).strip()
        end_str = range_match.group(3).strip()
        if start_str == 'å³æ—¥èµ·':
            start_date = datetime.datetime.now()
        else:
            start_str = start_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('.', '-').replace('/', '-')
            formats = ['%Y-%m-%d', '%m-%d']
            for fmt in formats:
                try:
                    if '%Y' not in fmt:
                        start_str = f"{datetime.datetime.now().year}-{start_str}"
                    start_date = datetime.datetime.strptime(start_str, fmt)
                    break
                except ValueError:
                    pass
        end_str = end_str.replace('å¹´', '-').replace('æœˆ', '-').replace('æ—¥', '').replace('.', '-').replace('/', '-').replace('ç‚¹', ':')
        formats = ['%Y-%m-%d %H:%M:%S', '%Y-%m-%d %H:%M', '%Y-%m-%d', '%m-%d %H:%M:%S', '%m-%d %H:%M', '%m-%d']
        for fmt in formats:
            try:
                if '%Y' not in fmt:
                    end_str = f"{datetime.datetime.now().year}-{end_str}"
                end_date = datetime.datetime.strptime(end_str, fmt)
                break
            except ValueError:
                pass
    return participants, prize, start_date, end_date

def print_summary_stats():
    """æ‰“å°çˆ¬è™«è¿è¡Œç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ğŸ¯ æ¯”èµ›ä¿¡æ¯çˆ¬è™«è¿è¡Œå®Œæˆ")
    print("="*60)
    print(f"â° è¿è¡Œæ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ”— é£ä¹¦è¡¨æ ¼: https://feishu.cn/base/{app_token}")
    print(f"ğŸ“Š è¡¨æ ¼ID: {table_id}")
    print("\nğŸ’¡ æç¤º:")
    print("  - æ•°æ®å·²è‡ªåŠ¨å»é‡ï¼Œé¿å…é‡å¤æ’å…¥")
    print("  - åªæ”¶é›†å½“å‰æœ‰æ•ˆçš„æ¯”èµ›ä¿¡æ¯")
    print("  - ç°å·²æ”¹ä¸ºé€ä¸ªå¯¼å…¥æ¨¡å¼ï¼Œæ¯æ¡æ•°æ®æ”¶é›†åç«‹å³æ’å…¥é£ä¹¦è¡¨æ ¼")
    print("  - å¦‚éœ€æŸ¥çœ‹è¯¦ç»†æ—¥å¿—ï¼Œè¯·æ£€æŸ¥æ§åˆ¶å°è¾“å‡º")
    print("="*60)

if __name__ == "__main__":
    start_time = datetime.datetime.now()
    print(f"ğŸš€ æ¯”èµ›ä¿¡æ¯çˆ¬è™«å¯åŠ¨ - {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    parser = argparse.ArgumentParser(
        description="æ¯”èµ›ä¿¡æ¯çˆ¬è™« - è‡ªåŠ¨çˆ¬å–å„å¹³å°æ¯”èµ›ä¿¡æ¯å¹¶æ’å…¥é£ä¹¦è¡¨æ ¼",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python crawler.py --platform baidu          # åªçˆ¬å–ç™¾åº¦AI Studio
  python crawler.py --platform all            # çˆ¬å–æ‰€æœ‰å¹³å°
  python crawler.py --platform wechat --biz <BIZ_ID> --token <TOKEN> --cookie <COOKIE>
        """
    )
    parser.add_argument("--platform", 
                       choices=['baidu', 'aliyun', 'wechat', 'tencent', 'all', 'update-status'], 
                       default='baidu', 
                       help="é€‰æ‹©çˆ¬å–å¹³å°: baidu(ç™¾åº¦AI Studio), aliyun(é˜¿é‡Œå¤©æ± ), wechat(å¾®ä¿¡å…¬ä¼—å·), tencent(è…¾è®¯CSDN), all(æ‰€æœ‰å¹³å°), update-status(æ›´æ–°æ¯”èµ›çŠ¶æ€)")
    parser.add_argument("--biz", help="å¾®ä¿¡å…¬ä¼—å· biz ID")
    parser.add_argument("--token", default="", help="å¾®ä¿¡å…¬ä¼—å· token")
    parser.add_argument("--cookie", default="", help="å¾®ä¿¡å…¬ä¼—å· cookie")
    parser.add_argument("--batch-size", type=int, default=10, help="æ‰¹é‡æ’å…¥å¤§å° (é»˜è®¤: 10)")
    parser.add_argument("--max-retries", type=int, default=3, help="æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)")
    
    args = parser.parse_args()
    
    try:
        if args.platform == 'baidu':
            crawl_baidu()
        elif args.platform == 'aliyun':
            crawl_aliyun()
        elif args.platform == 'wechat':
            if not args.biz:
                print("âŒ å¾®ä¿¡å…¬ä¼—å·çˆ¬å–éœ€è¦ --biz å‚æ•°")
                sys.exit(1)
            token = args.token or ""
            cookie = args.cookie or ""
            crawl_wechat(args.biz, token, cookie)
        elif args.platform == 'tencent':
            crawl_tencent()
        elif args.platform == 'update-status':
            print("\n" + "="*50)
            print("ğŸ”„ å¼€å§‹æ›´æ–°æ‰€æœ‰æ¯”èµ›çŠ¶æ€")
            print("="*50)
            update_all_competition_status()
        elif args.platform == 'all':
            print("\n" + "="*50)
            print("ğŸ¯ å¼€å§‹å…¨å¹³å°æ¯”èµ›ä¿¡æ¯çˆ¬å–")
            print("="*50)
            
            print("\n=== ğŸ” çˆ¬å–ç™¾åº¦AI Studioæ¯”èµ›ä¿¡æ¯ ===")
            crawl_baidu()
            
            print("\n=== ğŸ” çˆ¬å–é˜¿é‡Œå¤©æ± æ¯”èµ›ä¿¡æ¯ ===")
            crawl_aliyun()
            
            print("\n=== ğŸ” çˆ¬å–è…¾è®¯CSDNåšå®¢æ¯”èµ›ä¿¡æ¯ ===")
            crawl_tencent()
            
            if args.biz and args.token and args.cookie:
                print("\n=== ğŸ” çˆ¬å–å¾®ä¿¡å…¬ä¼—å·æ–‡ç«  ===")
                crawl_wechat(args.biz, args.token, args.cookie)
            else:
                print("\nâš ï¸ è·³è¿‡å¾®ä¿¡å…¬ä¼—å·: ç¼ºå°‘å¿…è¦å‚æ•° (--biz, --token, --cookie)")
                
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        sys.exit(1)
    finally:
        end_time = datetime.datetime.now()
        duration = end_time - start_time
        print(f"\nâ±ï¸ æ€»è¿è¡Œæ—¶é—´: {duration}")
        print_summary_stats()