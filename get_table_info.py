#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è·å–é£ä¹¦è¡¨æ ¼ä¿¡æ¯è„šæœ¬
ç”¨äºæŸ¥çœ‹è¡¨æ ¼ä¸­å·²æœ‰çš„æ¯”èµ›ä¿¡æ¯ï¼Œé¿å…é‡å¤æ·»åŠ 
"""

import argparse
from feishu_api import get_all_records, get_feishu_token
from config import FEISHU_APP_TOKEN, FEISHU_TABLE_ID
import json
from datetime import datetime

def get_table_info(show_details=False, filter_platform=None, export_json=False):
    """
    è·å–é£ä¹¦è¡¨æ ¼ä¸­çš„æ‰€æœ‰æ¯”èµ›ä¿¡æ¯
    
    Args:
        show_details: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
        filter_platform: è¿‡æ»¤ç‰¹å®šå¹³å°
        export_json: æ˜¯å¦å¯¼å‡ºä¸ºJSONæ–‡ä»¶
    """
    print("ğŸ” æ­£åœ¨è·å–é£ä¹¦è¡¨æ ¼ä¿¡æ¯...")
    
    # è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ
    token = get_feishu_token()
    if not token:
        print("âŒ æ— æ³•è·å–é£ä¹¦è®¿é—®ä»¤ç‰Œ")
        return
    
    # è·å–æ‰€æœ‰è®°å½•
    records = get_all_records()
    if not records:
        print("âŒ æ— æ³•è·å–è¡¨æ ¼è®°å½•")
        return
    
    print(f"ğŸ“Š è¡¨æ ¼ä¸­å…±æœ‰ {len(records)} æ¡è®°å½•")
    
    # ç»Ÿè®¡ä¿¡æ¯
    platform_stats = {}
    status_stats = {}
    unique_titles = set()
    duplicate_titles = []
    
    filtered_records = []
    
    for record in records:
        fields = record.get('fields', {})
        title = fields.get('æ ‡é¢˜', 'æœªçŸ¥æ ‡é¢˜')
        platform = fields.get('å¹³å°', 'æœªçŸ¥å¹³å°')
        status = fields.get('çŠ¶æ€', 'æœªçŸ¥çŠ¶æ€')
        
        # å¹³å°è¿‡æ»¤
        if filter_platform and platform != filter_platform:
            continue
            
        filtered_records.append(record)
        
        # ç»Ÿè®¡å¹³å°åˆ†å¸ƒ
        platform_stats[platform] = platform_stats.get(platform, 0) + 1
        
        # ç»Ÿè®¡çŠ¶æ€åˆ†å¸ƒ
        status_stats[status] = status_stats.get(status, 0) + 1
        
        # æ£€æŸ¥é‡å¤æ ‡é¢˜
        if title in unique_titles:
            duplicate_titles.append(title)
        else:
            unique_titles.add(title)
    
    print("\nğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   æ€»è®°å½•æ•°: {len(filtered_records)}")
    print(f"   å”¯ä¸€æ ‡é¢˜æ•°: {len(unique_titles)}")
    print(f"   é‡å¤æ ‡é¢˜æ•°: {len(duplicate_titles)}")
    
    print("\nğŸ¢ å¹³å°åˆ†å¸ƒ:")
    for platform, count in sorted(platform_stats.items()):
        print(f"   {platform}: {count} æ¡")
    
    print("\nğŸ“Š çŠ¶æ€åˆ†å¸ƒ:")
    for status, count in sorted(status_stats.items()):
        print(f"   {status}: {count} æ¡")
    
    if duplicate_titles:
        print("\nâš ï¸ å‘ç°é‡å¤æ ‡é¢˜:")
        for title in set(duplicate_titles):
            print(f"   - {title}")
    
    if show_details:
        print("\nğŸ“‹ è¯¦ç»†è®°å½•ä¿¡æ¯:")
        for i, record in enumerate(filtered_records, 1):
            fields = record.get('fields', {})
            print(f"\n{i}. æ ‡é¢˜: {fields.get('æ ‡é¢˜', 'N/A')}")
            print(f"   å¹³å°: {fields.get('å¹³å°', 'N/A')}")
            print(f"   çŠ¶æ€: {fields.get('çŠ¶æ€', 'N/A')}")
            print(f"   ç±»å‹: {fields.get('ç±»å‹', 'N/A')}")
            print(f"   éš¾åº¦: {fields.get('éš¾åº¦', 'N/A')}")
            print(f"   å¼€å§‹æ—¶é—´: {fields.get('å¼€å§‹æ—¶é—´', 'N/A')}")
            print(f"   ç»“æŸæ—¶é—´: {fields.get('ç»“æŸæ—¶é—´', 'N/A')}")
            print(f"   é“¾æ¥: {fields.get('é“¾æ¥', 'N/A')}")
    
    if export_json:
        # å¯¼å‡ºä¸ºJSONæ–‡ä»¶
        export_data = {
            'export_time': datetime.now().isoformat(),
            'total_records': len(filtered_records),
            'unique_titles': len(unique_titles),
            'duplicate_count': len(duplicate_titles),
            'platform_stats': platform_stats,
            'status_stats': status_stats,
            'duplicate_titles': list(set(duplicate_titles)),
            'records': []
        }
        
        for record in filtered_records:
            fields = record.get('fields', {})
            export_data['records'].append({
                'title': fields.get('æ ‡é¢˜', ''),
                'platform': fields.get('å¹³å°', ''),
                'status': fields.get('çŠ¶æ€', ''),
                'type': fields.get('ç±»å‹', ''),
                'difficulty': fields.get('éš¾åº¦', ''),
                'start_time': fields.get('å¼€å§‹æ—¶é—´', ''),
                'end_time': fields.get('ç»“æŸæ—¶é—´', ''),
                'link': fields.get('é“¾æ¥', ''),
                'participants': fields.get('å‚ä¸äººæ•°', ''),
                'prize': fields.get('å¥–é‡‘', ''),
                'create_time': fields.get('åˆ›å»ºæ—¶é—´', '')
            })
        
        filename = f"table_export_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ æ•°æ®å·²å¯¼å‡ºåˆ°: {filename}")
    
    print(f"\nğŸ”— é£ä¹¦è¡¨æ ¼é“¾æ¥: https://feishu.cn/base/{FEISHU_APP_TOKEN}")

def check_duplicate_title(title):
    """
    æ£€æŸ¥æŒ‡å®šæ ‡é¢˜æ˜¯å¦å·²å­˜åœ¨äºè¡¨æ ¼ä¸­
    
    Args:
        title: è¦æ£€æŸ¥çš„æ¯”èµ›æ ‡é¢˜
    
    Returns:
        bool: å¦‚æœæ ‡é¢˜å·²å­˜åœ¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    print(f"ğŸ” æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤: {title}")
    
    records = get_all_records()
    if not records:
        print("âŒ æ— æ³•è·å–è¡¨æ ¼è®°å½•")
        return False
    
    for record in records:
        fields = record.get('fields', {})
        existing_title = fields.get('æ ‡é¢˜', '')
        if existing_title == title:
            print(f"âš ï¸ å‘ç°é‡å¤æ ‡é¢˜: {title}")
            print(f"   å¹³å°: {fields.get('å¹³å°', 'N/A')}")
            print(f"   çŠ¶æ€: {fields.get('çŠ¶æ€', 'N/A')}")
            print(f"   åˆ›å»ºæ—¶é—´: {fields.get('åˆ›å»ºæ—¶é—´', 'N/A')}")
            return True
    
    print(f"âœ… æ ‡é¢˜ä¸é‡å¤: {title}")
    return False

def main():
    parser = argparse.ArgumentParser(
        description="è·å–é£ä¹¦è¡¨æ ¼ä¿¡æ¯ï¼Œæ£€æŸ¥é‡å¤æ¯”èµ›",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python get_table_info.py                          # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
  python get_table_info.py --details               # æ˜¾ç¤ºè¯¦ç»†è®°å½•ä¿¡æ¯
  python get_table_info.py --platform baidu        # åªæ˜¾ç¤ºç™¾åº¦å¹³å°çš„è®°å½•
  python get_table_info.py --export                # å¯¼å‡ºæ•°æ®ä¸ºJSONæ–‡ä»¶
  python get_table_info.py --check-title "æ¯”èµ›åç§°"  # æ£€æŸ¥æŒ‡å®šæ ‡é¢˜æ˜¯å¦é‡å¤
        """
    )
    
    parser.add_argument("--details", action="store_true", help="æ˜¾ç¤ºè¯¦ç»†è®°å½•ä¿¡æ¯")
    parser.add_argument("--platform", choices=['baidu', 'aliyun', 'wechat', 'tencent'], help="è¿‡æ»¤ç‰¹å®šå¹³å°")
    parser.add_argument("--export", action="store_true", help="å¯¼å‡ºæ•°æ®ä¸ºJSONæ–‡ä»¶")
    parser.add_argument("--check-title", help="æ£€æŸ¥æŒ‡å®šæ ‡é¢˜æ˜¯å¦å·²å­˜åœ¨")
    
    args = parser.parse_args()
    
    try:
        if args.check_title:
            check_duplicate_title(args.check_title)
        else:
            get_table_info(
                show_details=args.details,
                filter_platform=args.platform,
                export_json=args.export
            )
    except KeyboardInterrupt:
        print("\nâŒ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()